CSE 415 Big Boi Topics
*** = hella important
** = good to note

State Spaces (Sigma) = set of all possible states
- move = transion of states
- operator (state transformation + pre-conditional/predicate)
- problem space = state space + operators
	- the same state space can be part of multiple prob. spaces
	- ex: prob space = {1,2,3,4} operators --> operator add1
	- problem space graphs: undirected graph showing solution path
	- quiz example: Sum(i=0,7)[Choose from 7 pieces * Choose from 7 
	places*i pieces!*(6^i) orientations^pieces]

Combinatorics
- combinatorial explosion = expon. growth
	- b moves, algorithm looks n steps ahead, 1+b+b^2...b^n states
- **Painted Squares Puzzle example:
	- n_permutations * n_orientations^n_pieces possible states in a game

Searchs
- BFS preferred over DFS, but can be expensive
- iteratiive deepening DFS = benefits from both DFS+BFS, has depth limits++
- ItrDFS saves memory at expense of time
- Dijisktra/UCS = searchs nodes in order of increasing path cost from start
	- Best First Search: same as UCS but with PQ

Heuristic Search
- admissible heurist.= never overestimates true cost to reach goal node from any node
	- A* finds shortest path as soon as it expands the goal.
- consistent = heuristic value of n <= cost of its successor n′ + successor's heuristic
	- A* never has to re-expand a node
	- h values monotonically decreasing along shortest path to goal

Adversarial Search
- Mini-max Search: each player (one trying to get max score, one trying to get min)
	- max player = triangle pointing up
	- DEPTH FIRST TRAVERSAL (go all the way bottom left, then go right)
	- Each agent will ALWAYS choose the best choice
	- Calculation tips: start from bottom of tree, bring values upward
	- Alpha-beta Pruning: set (-infinity, infinity), where max is -infinity (start
	from extremely small value)
		- everytime you hit a leaf node, assess value (either max or min depending
			on whatever triangle you last went down) and pass upward
		- if alpha >= beta (max player crosses value of min player), STOP! Prune/forget
		the remaining branches under the triangle.
		- sometimes this algorithms has max depth search/iterative deepening
- **Expectimax Search: other player plays randomly/illogically
	- random player = circle
	- take sum of weighted average of possibilities (probability * leaf node value)
	- CANNOT PRUNE

Markov Decision Processes
- Plans can't go wrong; know every action from start --> end
- Policy: tells preferred actions, is characteristic of agent
- Discount: how much your reward changes per step (discount (gamma < 1) or living reward, or stay same)
- Q-Value: Q1(s,left) = agent commits to taking a FIRST ACTION! Then Q value = the optimal score from then on.
- ***Bellman Equations: Key: V*(s) is the expected value if you take infinite optimal actions in a game. Q*(s,a) is the expected value if you take one certain first action, then go anywhere infinitely after that. R(s,a,s') = reward you get going from s to s', taking action a.
	- V*(s) = maxA Q*(s,a)
		- given A within all possible actions, choose the best choice, V = Q
	- Q*(s,a) = SUM|A T(s,a,s') * [R(s,a,s')+GAMMA(V*(s'))]
		- given end state s', find the sum of weighted averages
		- Transition function = probability of the action happening
		- Reward = value you get for taking that action
		- GAMMA = discount
		- V*(s') part of recursive definition of Bellman, get V value from children
- Temporal Difference Learning: agent learns from each experience
	- update V(s) each time we experience a transition
	- likely outcomes s' will contribute to updates more often
	- policy is still fixed, still doing evaluations
	- "Learning Rate" = how fast the agent learns new thing AND forgets old value, aka ALPHA
		- Exponential Moving Average (makes recent samples more important)
		- Decreasing Alpha Learning Rate (earlier actions have more impact)
		- Example use: Vpi(s) = (1-ALPHA)*Vpi(s) + ALPHA(new sample)
			- (1-0.5)*0 + 0.5[-2 + 1.0*8] = 3, 0 = previous V, 0.5 = learning rate, -2 = reward, 1.0 = gamma, Vpi(s') = 8
	- problems with TD learning: if we want to turn values into new policy, we don't have the needed Q values
- ***Q-Learning: Basically Bellman Equations, but... the sample is (Reward + GAMMA*[max|A' Qk(s',a')])
	- considers the successors' Q-values instead of just Value, learn Q-Values as you go
	- "Off-policy learning" = Q-learning will converge to the optimal policy, even if acting suboptimally
	- However... need to explore many many states, Epsilon greedy --> random actions like flipping coin every step, learn first exploit later.
		- takes a long time, learning rate small
		- See Exp F(x) to fix the above issues!
- **Exploration Function: tells agent when to explore? Explore places w/ unestablished "badness" values first!
	- input value estimate "U" and visit count "N", output optimistic utility
		- f(U,N) = U + K/N, where K is a constant and K/N gets larger over time
	- modified Q-Update: Q(s,a) = R(s,a,s') + GAMMA*max|A' f[Q(s,a'),N(s',a')]
		- Reward + discounted new sample value where the maximum action A is chosen and the exploration function is applied w/ the Q-value and visit count of that state
	- Regret: measure of total mistake costs = expected rewards - optimal rewards
		- optimally learning to be optimal
		- exploration function's Regret < random exploration regret
- Feature Based Reinforcement Learning: want to generalize actions as opposed to normal Q-learning
	- learn about small number of training states from experience
	- Q-learning uses too much memory. FBR is better.
		- ex: Pacman game, feature = distance from self to closest ghosts, whether they are in tunnel, distance to food...etc
	- Solution: State = vector of properties/features
		- function that converts state to real number
		- ex: number of enemies, distance to goal
	- Linear Value Functions: write a Q function for any state using a few weights (w1,w2...)
		- pros: state can be summed up by a number
		- cons: multiple states might share a number
		- modified Q-Learning Equation: Q(s,a) = w1*f1(s,a)+w2*f2(s,a)... + wn*fn(s,a)
	- if something bad happens, blame the features that were on and disprefer those states
	
Probabilistic Reasoning
- why? data = noise, models uncertainty
- P(X), where X is a random variable. A symbol representing class of events that may occur at any number of times, takes on value in given set D domain.
	- event = "Sunny on Thursday", Class of event: weather
- Probability distribution: P=0~1, function that assigns to each domain element a value, if D is finite, then P is usually a table. Table numbers must add up to 1.0
- Domain: "Sun", "Rain", "Fog"
- Example P. Distrib = "0.6, 0.1, 0.3..."
- Event: Set E of all outcomes
- Bayes' Rule: P(A|B) = P(B|A)*P(A)/(P(B)
- Bayes' Networks: graph showing joint distributions of probabilities
	- can be efficient, take advantage of independence, conditional independence
	- probabilistic interference
	- compute desired prob. from other known probs.
	- Ek... evidence variables
	- Q = query variable
	- Hidden variables = H1... Hr
	- inference by ENUMERATION (we want P(Q|e1...ek), e being results)
		1. Select entries in table consistent with evidence
		2. Sum out H to get joint of Query and evidence
		3. Normalize (1/Z)
	- Marginal Probability = BEFORE you know the evidence (P(Q)… don't know)
	- Posterior Probability = AFTER evidence
	- COMPUTATIONAL COMPLEXITY
		- Time complexity = O(d^n), d = number of random variables, n entries
		- Space complexity is the same, to store the joint distribution

Neural Networks
- Perceptron: flowchart starting with inputs and their weights (w1,w2...) on the left, threshold, optional hidden nodes, output on the right.
	- computationally powerful, static
	- can allow for tolerating mismatch (lower the threshold value)
- Perceptron Training Sets
	- X = X, U, X-… set of training examples
	- Sx = (X1,X2,X3…) is a training sequence on X, given
		○ Each Xk is a member of X
		○ Each element of X occurs infinitely often in Sx
	- You should have a mix of each training example in your training sequence
	- You can never run out of one example
		○ must keep recurring (infinitely often)
		○ you might need more occurrences to converge
	- Prevents this algorithm from never seeing a training example
	- Limitations... can't compute Exclusive OR? Use multi-layer perceptrons.
	- ***multi-layer perceptrons: have multiple thresholds, multiple inputs going to multiple paths, maybe has hidden nodes.
	- Hidden Node Input Activation: sigmoid activation functions, does not apply threshold! 
- RLU (Rectified Linear Unit)
	- continuous graph
	- 0 if x < 0, a straight diagonal line any x > 0
	- always increasing, pretty good! (only if x>0 tho)
- *** Epoch: 1 pass through the training set, with an adjustment to networks weights for each training example.
- ***If not enough hidden nodes, training might not converge!
- ***DELTA RULE: for each training example <X,T>, compute F(X), the outpuss based on current weights. Add DELTAw to a weight when updating: DELTAw = N*d*F, N = training rate.
	- if W leads to hidden node, use backpropagation.

Deep Learning
- Hill Climbing concept: start wherever, repeatedly move to best neighboring state, if no neighbors better than current --> QUIT
	- something could go wrong: might encounter a "plateau", or you found a local maximum, thinking its the highest point
	- How to find a good local optimum?
- Auto-encoder (crude idea sketch)
	- encodes the image input
	- learn something from even unlabeled images, do some filtering, produce an output (same number as inputs)
		- train to match output with input!!! If something doesn't match, then use that as an error term and backpropagate.
- Training Procedure: Stacked Auto-Encoder, learning many layers 
	- auto-encoder only uses 1 layer, "compressed" version of input layer
	- stacked: LAYER1 = compress every image, LAYER2 = using compressed images as input, and as output to be predicted, repeat for LAYER3,4,5...
		- note: between layers, responses get agglomerated from several neurons ("complex cells")
- Deep Learning is generally harder to understand what the network represents at each step compared to Tree-based, Reinforced Learning...






