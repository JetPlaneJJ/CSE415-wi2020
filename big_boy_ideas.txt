CSE 415 Big Boi Topics
*** = hella important
** = good to note

State Spaces (Sigma) = set of all possible states
- move = transion of states
- operator (state transformation + pre-conditional/predicate)
- problem space = state space + operators
	- the same state space can be part of multiple prob. spaces
	- ex: prob space = {1,2,3,4} operators --> operator add1
	- problem space graphs: undirected graph showing solution path
	- quiz example: Sum(i=0,7)[Choose from 7 pieces * Choose from 7 
	places*i pieces!*(6^i) orientations^pieces]

Combinatorics
- combinatorial explosion = expon. growth
	- b moves, algorithm looks n steps ahead, 1+b+b^2...b^n states
- **Painted Squares Puzzle example:
	- n_permutations * n_orientations^n_pieces possible states in a game

Searchs
- BFS preferred over DFS, but can be expensive
- iteratiive deepening DFS = benefits from both DFS+BFS, has depth limits++
- ItrDFS saves memory at expense of time
- Dijisktra/UCS = searchs nodes in order of increasing path cost from start
	- Best First Search: same as UCS but with PQ

Heuristic Search
- admissible heurist.= never overestimates true cost to reach goal node from any node
	- A* finds shortest path as soon as it expands the goal.
- consistent = heuristic value of n <= cost of its successor nâ€² + successor's heuristic
	- A* never has to re-expand a node
	- h values monotonically decreasing along shortest path to goal

Adversarial Search
- Mini-max Search: each player (one trying to get max score, one trying to get min)
	- max player = triangle pointing up
	- DEPTH FIRST TRAVERSAL (go all the way bottom left, then go right)
	- Each agent will ALWAYS choose the best choice
	- Calculation tips: start from bottom of tree, bring values upward
	- Alpha-beta Pruning: set (-infinity, infinity), where max is -infinity (start
	from extremely small value)
		- everytime you hit a leaf node, assess value (either max or min depending
			on whatever triangle you last went down) and pass upward
		- if alpha >= beta (max player crosses value of min player), STOP! Prune/forget
		the remaining branches under the triangle.
		- sometimes this algorithms has max depth search/iterative deepening
- **Expectimax Search: other player plays randomly/illogically
	- random player = circle
	- take sum of weighted average of possibilities (probability * leaf node value)
	- CANNOT PRUNE

Markov Decision Processes
- Plans can't go wrong; know every action from start --> end
- Policy: tells preferred actions, is characteristic of agent
- Discount: how much your reward changes per step (discount (gamma < 1) or living reward, or stay same)
- Q-Value: Q1(s,left) = agent commits to taking a FIRST ACTION! Then Q value = the optimal score from then on.
- ***Bellman Equations: Key: V*(s) is the expected value if you take infinite optimal actions in a game. Q*(s,a) is the expected value if you take one certain first action, then go anywhere infinitely after that. R(s,a,s') = reward you get going from s to s', taking action a.
	- V*(s) = maxA Q*(s,a)
		- given A within all possible actions, choose the best choice, V = Q
	- Q*(s,a) = SUM|A T(s,a,s') * [R(s,a,s')+GAMMA(V*(s'))]
		- given end state s', find the sum of weighted averages
		- Transition function = probability of the action happening
		- Reward = value you get for taking that action
		- GAMMA = discount
		- V*(s') part of recursive definition of Bellman, get V value from children
- Temporal Difference Learning: agent learns from each experience
	- update V(s) each time we experience a transition
	- likely outcomes s' will contribute to updates more often
	- policy is still fixed, still doing evaluations
	- "Learning Rate" = how fast the agent learns new thing AND forgets old value, aka ALPHA
		- Exponential Moving Average (makes recent samples more important)
		- Decreasing Alpha Learning Rate (earlier actions have more impact)
		- Example use: Vpi(s) = (1-ALPHA)*Vpi(s) + ALPHA(new sample)
			- (1-0.5)*0 + 0.5[-2 + 1.0*8] = 3, 0 = previous V, 0.5 = learning rate, -2 = reward, 1.0 = gamma, Vpi(s') = 8
	- problems with TD learning: if we want to turn values into new policy, we don't have the needed Q values
- ***Q-Learning: Basically Bellman Equations, but... the sample is (Reward + GAMMA*[max|A' Qk(s',a')])
	- considers the successors' Q-values instead of just Value, learn Q-Values as you go
	- "Off-policy learning" = Q-learning will converge to the optimal policy, even if acting suboptimally
	- However... need to explore many many states, Epsilon greedy --> random actions like flipping coin every step, learn first exploit later.
		- takes a long time, learning rate small
		- See Exp F(x) to fix the above issues!
- **Exploration Function: tells agent when to explore? Explore places w/ unestablished "badness" values first!
	- input value estimate "U" and visit count "N", output optimistic utility
		- f(U,N) = U + K/N, where K is a constant and K/N gets larger over time
	- modified Q-Update: Q(s,a) = R(s,a,s') + GAMMA*max|A' f[Q(s,a'),N(s',a')]
		- Reward + discounted new sample value where the maximum action A is chosen and the exploration function is applied w/ the Q-value and visit count of that state
	- Regret: measure of total mistake costs = expected rewards - optimal rewards
		- optimally learning to be optimal
		- exploration function's Regret < random exploration regret
- Feature Based Reinforcement Learning: want to generalize actions as opposed to normal Q-learning
	- learn about small number of training states from experience
	- Q-learning uses too much memory. FBR is better.
		- ex: Pacman game, feature = distance from self to closest ghosts, whether they are in tunnel, distance to food...etc
	- Solution: State = vector of properties/features
		- function that converts state to real number
		- ex: number of enemies, distance to goal
	- Linear Value Functions: write a Q function for any state using a few weights (w1,w2...)
		- pros: state can be summed up by a number
		- cons: multiple states might share a number
		- modified Q-Learning Equation: Q(s,a) = w1*f1(s,a)+w2*f2(s,a)... + wn*fn(s,a)
	- if something bad happens, blame the features that were on and disprefer those states
	





