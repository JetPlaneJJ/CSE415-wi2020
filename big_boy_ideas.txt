CSE 415 Big Boi Topics
*** = hella important
** = good to note

State Spaces (Sigma) = set of all possible states
- move = transion of states
- operator (state transformation + pre-conditional/predicate)
- problem space = state space + operators
	- the same state space can be part of multiple prob. spaces
	- ex: prob space = {1,2,3,4} operators --> operator add1
	- problem space graphs: undirected graph showing solution path
	- quiz example: Sum(i=0,7)[Choose from 7 pieces * Choose from 7 
	places*i pieces!*(6^i) orientations^pieces]

Combinatorics
- combinatorial explosion = expon. growth
	- b moves, algorithm looks n steps ahead, 1+b+b^2...b^n states
- **Painted Squares Puzzle example:
	- n_permutations * n_orientations^n_pieces possible states in a game

Searchs
- BFS preferred over DFS, but can be expensive
- iteratiive deepening DFS = benefits from both DFS+BFS, has depth limits++
- ItrDFS saves memory at expense of time
- Dijisktra/UCS = searchs nodes in order of increasing path cost from start
	- Best First Search: same as UCS but with PQ

Heuristic Search
- admissible heurist.= never overestimates true cost to reach goal node from any node
	- A* finds shortest path as soon as it expands the goal.
- consistent = heuristic value of n <= cost of its successor nâ€² + successor's heuristic
	- A* never has to re-expand a node
	- h values monotonically decreasing along shortest path to goal

Adversarial Search
- Mini-max Search: each player (one trying to get max score, one trying to get min)
	- max player = triangle pointing up
	- DEPTH FIRST TRAVERSAL (go all the way bottom left, then go right)
	- Each agent will ALWAYS choose the best choice
	- Calculation tips: start from bottom of tree, bring values upward
	- Alpha-beta Pruning: set (-infinity, infinity), where max is -infinity (start
	from extremely small value)
		- everytime you hit a leaf node, assess value (either max or min depending
			on whatever triangle you last went down) and pass upward
		- if alpha >= beta (max player crosses value of min player), STOP! Prune/forget
		the remaining branches under the triangle.
		- sometimes this algorithms has max depth search/iterative deepening
- **Expectimax Search: other player plays randomly/illogically
	- random player = circle
	- take sum of weighted average of possibilities (probability * leaf node value)
	- CANNOT PRUNE

Markov Decision Processes
- Plans can't go wrong; know every action from start --> end
- Policy: tells preferred actions, is characteristic of agent
- Discount: how much your reward changes per step (discount (gamma < 1) or living reward, or stay same)
- Q-Value: Q1(s,left) = agent commits to taking a FIRST ACTION! Then Q value = the optimal score from then on.
- ***Bellman Equations: Key: V*(s) is the expected value if you take infinite optimal actions in a game. Q*(s,a) is the expected value if you take one certain first action, then go anywhere infinitely after that. R(s,a,s') = reward you get going from s to s', taking action a.
	- V*(s) = maxA Q*(s,a)
		- given A within all possible actions, the best choice, V = Q
		- 



